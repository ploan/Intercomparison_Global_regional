{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6230f6e-236c-434d-83eb-0463cda4c984",
   "metadata": {},
   "source": [
    "# Master function for Lisa's paper on inter-comparison precipitation extremes at regional scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e04456a6-e095-42b0-8335-bbf18fbc633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules \n",
    "import pandas as pd\n",
    "import os\n",
    "import fnmatch\n",
    "import xarray as xr\n",
    "import xskillscore as xs\n",
    "import numpy as np\n",
    "import math\n",
    "#from scipy import stats\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.stats import mannwhitneyu\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04bc00d-6bf4-4576-ad20-f6ec3bf32b86",
   "metadata": {},
   "source": [
    "### Define names of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30c0bdb4-6a03-409d-b5e1-1ddd2efd7916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List names of all precipitation datasets\n",
    "data_names = [\"3B42_IR_v7.0\", \"3B42_MW_v7.0\", \"3B42RT_UNCAL_v7.0\", \"3B42RT_v7.0\",\"3B42_v7.0\", \"ARC2\", \"CFSR\", \"CHIRPS_v2.0\", \"CHIRP_V1\", \"CMORPH_v1.0_CRT\", \"CMORPH_v1.0_RAW\",\n",
    "            \"COSCH\", \"CPC_v1.0\", \"ERA5\", \"ERAi\", \"GPCC_FDD_v1.0\", \"GPCC_FDD_v2018\", \"GPCC_FDD_v2020\", \"GPCC_FDD_v2022\", \"GPCC_FG_v1.0\", \"GPCP_CDR_v1.3_not-enforced\", \"GPCP_CDR_v1.3_yes-enforced\", \n",
    "             \"GPCP_IP\", \"GPCP_V3.2\", \"GSMAP-gauges-NRT-v6.0\", \"GSMAP-nogauges-NRT-v6.0\", \"GSWP3\", \"HOAPSv4.0\", \"IMERG_V06_EU\", \"IMERG_V06_FC\", \"IMERG_V06_FU\", \"IMERG_V06_LU\", \"JRA-55\", \"MERRA1\", \n",
    "             \"MERRA2\", \"PERSIANN_CCS_CDR\", \"PERSIANN_v1_r1\", \"REGEN_ALL_2019\", \"REGEN_LONG_V1\", \"SM2RAIN-ASCAT\", \"TAMSAT_v3.1\", \"TAMSAT_v3\", \"TAPEER_v1.5\", \"GSMAP-NRT-gauges-v8.0\", \"IMERG-v07B-FC\", \"GIRAFE\", \n",
    "              \"IMERG_V07B_FC\", \"GsMAP-gauges-NRT-v8\", \"PERSIANN_v1_r1\", \"CFSR\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d6063f-0948-4708-9a7a-0a1d9daa0e87",
   "metadata": {},
   "source": [
    "### Get Model Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d719694-8bd4-4404-a816-c5eefa40a5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the dictionaries above to get the dataset names from each file\n",
    "def get_data_name(names, file):\n",
    "    \n",
    "    # Check the model name is in one of the above data lists\n",
    "    for name in names:\n",
    "        if fnmatch.fnmatch(file, \"*\" + name + \"_*\"):\n",
    "            return name\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61af3913-3d27-4496-89fd-4f56f19656f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model file paths and add to a Pandas Dataframe \n",
    "def get_data_files(data_master_path):\n",
    "    \n",
    "    # Initialise pandas df of model_paths\n",
    "    data_paths = pd.DataFrame(columns=['dataset', 'dataset_path'])\n",
    "\n",
    "    # Loop through the data directory passed to the function, check that the file is a model in the above lists, and get the dataset paths\n",
    "    for data_file in os.listdir(data_master_path):\n",
    "    \n",
    "        # Get the RCM and GCM files names\n",
    "        data_name = get_data_name(data_names, data_file)\n",
    "    \n",
    "        # Get full path to the dataset \n",
    "        data_complete_file_path = data_master_path + data_file\n",
    "            \n",
    "        # Add information to DataFrame\n",
    "        data_paths.loc[len(data_paths.index)] = [data_name, data_complete_file_path]\n",
    "            \n",
    "    return data_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8614c4bb-aa26-427b-b2b5-75c5293b47e4",
   "metadata": {},
   "source": [
    "### Get Model Data Paths for Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81ec9386-dfb0-461b-a514-5e14bad2acad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model file paths for subset of models from list defined above\n",
    "def get_data_files_subset(data_paths, subset_names):\n",
    "    \n",
    "    # Initialize new Pandas Dataframe to hold data paths for subsets of models\n",
    "    data_paths_subset = pd.DataFrame(columns=['data_name', 'dataset_path'])\n",
    "\n",
    "    for i, row in data_paths.iterrows():\n",
    "        data_name = f'{row[0]}'\n",
    "    \n",
    "        if data_name in subset_names:\n",
    "            data_paths_subset.loc[len(data_paths_subset.index)] = [data_name, f'{row[1]}']\n",
    "        \n",
    "    return data_paths_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e180dda-d519-44a5-b28a-bbb17a547ec9",
   "metadata": {},
   "source": [
    "### Get Data from File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fbf0771-cf91-459c-aa9b-786a9b8e4386",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract data from files based on user specifications \n",
    "def get_data_from_file(file_path, variable,lat_slice, lon_slice, season=None, iscale=None):\n",
    "    \n",
    "    # If iscale is not given, check other variable exceptions before extracting data\n",
    "    if iscale is None:\n",
    "    \n",
    "        # If the variable is CDD/CWD (Consecutive Dry/Wet Days) or R*mm (number of heavy rain days), the data must be read in differently to convert the variable dtype to float\n",
    "        if variable == 'cdd':\n",
    "        \n",
    "            data_ds = xr.open_dataset(file_path, decode_cf=False)\n",
    "            data_ds.cdd.attrs['units'] = '1'\n",
    "            data_ds = xr.decode_cf(data_ds)\n",
    "            data_ds.cdd.attrs['units'] = 'days'\n",
    "            data_var = data_ds.cdd.sel(lat=lat_slice, lon=lon_slice)\n",
    "            \n",
    "        elif variable == 'cwd':\n",
    "        \n",
    "            data_ds = xr.open_dataset(file_path, decode_cf=False)\n",
    "            data_ds.cwd.attrs['units'] = '1'\n",
    "            data_ds = xr.decode_cf(data_ds)\n",
    "            data_ds.cwd.attrs['units'] = 'days'\n",
    "            data_var = data_ds.cwd.sel(lat=lat_slice, lon=lon_slice)\n",
    "    \n",
    "        elif variable == 'r10mm':\n",
    "        \n",
    "            data_ds = xr.open_dataset(file_path, decode_cf=False)\n",
    "            data_ds.r10mm.attrs['units'] = '1'\n",
    "            data_ds = xr.decode_cf(data_ds)\n",
    "            data_ds.r10mm.attrs['units'] = 'days'\n",
    "            data_var = data_ds.r10mm.sel(lat=lat_slice, lon=lon_slice)\n",
    "        \n",
    "        elif variable == 'r20mm':\n",
    "        \n",
    "            data_ds = xr.open_dataset(file_path, decode_cf=False)\n",
    "            data_ds.r20mm.attrs['units'] = '1'\n",
    "            data_ds = xr.decode_cf(data_ds)\n",
    "            data_ds.r20mm.attrs['units'] = 'days'\n",
    "            data_var = data_ds.r20mm.sel(lat=lat_slice, lon=lon_slice)\n",
    "        \n",
    "        elif variable == 'r30mm':\n",
    "        \n",
    "            data_ds = xr.open_dataset(file_path, decode_cf=False)\n",
    "            data_ds.r30mm.attrs['units'] = '1'\n",
    "            data_ds = xr.decode_cf(data_ds)\n",
    "            data_ds.r30mm.attrs['units'] = 'days'\n",
    "            data_var = data_ds.r30mm.sel(lat=lat_slice, lon=lon_slice)\n",
    "            \n",
    "        elif variable == 'fracprday':\n",
    "            \n",
    "            data_ds = xr.open_dataset(file_path, decode_cf=False)\n",
    "            data_ds.fracprday.attrs['units'] = '1'\n",
    "            data_ds = xr.decode_cf(data_ds)\n",
    "            data_ds.fracprday.attrs['units'] = 'days'\n",
    "            data_var = data_ds.fracprday.sel(lat=lat_slice, lon=lon_slice)\n",
    "            \n",
    "        elif variable == 'tas':\n",
    "            data_ds = xr.open_dataset(file_path)\n",
    "            data_var=data_ds.tas.sel(lat=lat_slice, lon=lon_slice)\n",
    "            \n",
    "    \n",
    "        else:\n",
    "    \n",
    "            # Open file and store data as a DataArray\n",
    "            data_ds = xr.open_dataset(file_path)\n",
    "\n",
    "            # Subset data based on user-specified spatiotemporal boundaries\n",
    "            data_subset = data_ds.sel(lat=lat_slice, lon=lon_slice)\n",
    "    \n",
    "            # Extract user-specified variable\n",
    "        \n",
    "            data_var = getattr(data_subset, variable)\n",
    "   \n",
    "    # Extract appropriate SPI data based on the user-defined scale if the iscale is not None\n",
    "    # (1 for 3-month averaging period, 2 for 6-month averaging period, 3 for 12-month averaging period)\n",
    "    else:\n",
    "        \n",
    "        # Extract user-specified SPI data\n",
    "        data_ds = xr.open_dataset(file_path)\n",
    "        data_var = data_ds.spi.sel(lat=lat_slice, lon=lon_slice)\n",
    "        \n",
    "    if season is None: \n",
    "        pass\n",
    "        \n",
    "    else:\n",
    "        data_var = data_var.sel(time=data_var.time.dt.month.isin(season))\n",
    "    \n",
    "    return data_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17650706-3258-4a97-a7d3-28cec8c5c301",
   "metadata": {},
   "source": [
    "## Spatially Averaged Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79be6154-1c1a-4bd1-828e-e92584304b0b",
   "metadata": {},
   "source": [
    "The following functions calculate spatially averaged metrics, primarily for plotting time series and scatterplots. <br>\n",
    "'season=None' sets an optional argument for seasonal subsetting based on numeric month values <br>\n",
    "'mask=None' sets an optional argument for masking, where the default is None <br>\n",
    "'scale=None' sets an optional argument for selecting the scale of the 4-D SPI indice. This should only be assigned a value <br>\n",
    "    if the variable is SPI. The scale can be 3, 6, or 12 months. <br>\n",
    "**Functions Include:** <br>\n",
    "- Weighted Spatial Mean at Default Time Step\n",
    "- Monthly Averages over time/space to gauge seasonality (Homogenous Variable Names, like Climpact)\n",
    "- Annual Averages\n",
    "- Annual Time Series\n",
    "- Weighted Spatial Average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321b260c-e2a5-4b8d-b1d4-fd6c11ea28ab",
   "metadata": {},
   "source": [
    "### Weighted Spatial Mean at Default Time Step (No Temporal Averaging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e9580f4-5b8b-4c11-8de6-a90f4e2a7efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get monthly averages to use for a line plot over a user-specified time period and spatial domain\n",
    "def get_weighted_spatial_average_at_default_time_step(data_path, variable, lat_slice, lon_slice, season=None, iscale=None, mask=None):\n",
    "    \n",
    "    # Get data\n",
    "    # Include SPI scale in data extraction if a scale is provided by the user\n",
    "    if iscale is None:\n",
    "        data = get_data_from_file(data_path, variable, time_slice, lat_slice, lon_slice, season)\n",
    "    \n",
    "    else:\n",
    "        data = get_data_from_file(data_path, variable, time_slice, lat_slice, lon_slice, season, iscale)\n",
    "              \n",
    "    # Extract unmasked data if no mask provided\n",
    "    if mask is None:\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "       \n",
    "        # Apply mask to data\n",
    "        data = data.where(mask==1)\n",
    "    \n",
    "    # Compute latitudinally weighted spatially averaged data\n",
    "    # Create latitudinal weights\n",
    "    weights = np.cos(np.deg2rad(data.lat))\n",
    "    weights.name = \"weights\"\n",
    "\n",
    "    # Weight: apply weighted lats to data\n",
    "    data_weighted = data.weighted(weights)\n",
    "\n",
    "    # Get weighted averages at each time step\n",
    "    data_weighted_mean = data_weighted.mean(('lon', 'lat'))\n",
    "    #data_weighted_mean = data.mean(('longitude', 'latitude'))\n",
    "    \n",
    "    # Return weighted spatial average at default time step\n",
    "    return data_weighted_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee51af2e-8f65-4f23-8266-2699e3b0738a",
   "metadata": {},
   "source": [
    "### Get Annual Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98a53ef7-dea0-4f88-8215-5f9c3cd94975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get annual averages to use for a line plot over a user-specified time period and spatial domain\n",
    "def get_annual_average(data_path, variable, lat_slice, lon_slice, iscale=None, mask=None, region_mask=None, region=None):\n",
    "    \n",
    "    # Get data\n",
    "    # Include SPI scale in data extraction if a scale is provided by the user\n",
    "    if iscale is None:\n",
    "        data = get_data_from_file(data_path, variable, lat_slice, lon_slice)\n",
    "    \n",
    "    else:\n",
    "        data = get_data_from_file(data_path, variable,lat_slice, lon_slice, iscale)\n",
    "    \n",
    "    # Check if we are doing an IPCC sub-region\n",
    "    if region is None:\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        data = data.where(region_mask==region)\n",
    "    \n",
    "    # Extract unmasked data if no mask provided\n",
    "    if mask is None:\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "       \n",
    "        # Apply mask to data\n",
    "        data = data.where(mask==1)\n",
    "    \n",
    "    # Compute latitudinally weighted spatially averaged data\n",
    "    # Create latitudinal weights\n",
    "    weights = np.cos(np.deg2rad(data.lat))\n",
    "    weights.name = \"weights\"\n",
    "\n",
    "    # Weight: apply weighted lats to data\n",
    "    data_weighted = data.weighted(weights)\n",
    "\n",
    "    # Get weighted averages at each time step\n",
    "    data_weighted_mean = data_weighted.mean(('lon', 'lat'))\n",
    "\n",
    "    # Get weighted monthly averages\n",
    "    data_weighted_mean_ann = data_weighted_mean.groupby('time.year').mean(dim='time')\n",
    "    \n",
    "    return data_weighted_mean_ann"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8af0a60-b81e-469e-92d9-30449e4db8c9",
   "metadata": {},
   "source": [
    "## Plotting Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a652f2-ad39-4210-8040-a0e11dfd0c3d",
   "metadata": {},
   "source": [
    "### Truncate a color map (skip colors in pre-defined colormap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "194b40ac-9154-43f9-af85-93ffdb6a725b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to be able to skip colors/subset colors within a predefined colormap\n",
    "def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n",
    "    new_cmap = colors.LinearSegmentedColormap.from_list(\n",
    "        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
    "        cmap(np.linspace(minval, maxval, n)))\n",
    "    return new_cmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41c3d41-9759-4b0b-8d31-1f3074218e37",
   "metadata": {},
   "source": [
    "### Define vertical bands (regions) to shade along a time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "219826b3-9dd3-4265-b049-9ffba64b31f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to find areas that need to be shaded using a boolean mask as input\n",
    "def fill_vertical_columns(boolean_fill_mask):\n",
    "    \n",
    "    # Find areas in the mask when values change (i.e. boolean switched from True to False and vice versa)\n",
    "    boolean_switch = np.diff(boolean_fill_mask)\n",
    "    \n",
    "    # Find start and end of sections where the boolean fill mask is True (places I want to shade)\n",
    "    region_to_shade, = boolean_switch.nonzero()\n",
    "    \n",
    "    # Handle edge cases where condition starts or ends with True\n",
    "    if boolean_fill_mask[0]:\n",
    "        region_to_shade = np.r_[0, region_to_shade]\n",
    "   \n",
    "    if boolean_fill_mask[-1]:\n",
    "        region_to_shade = np.r_[region_to_shade, len(boolean_fill_mask)]\n",
    "    \n",
    "    # Reshape the result into pairs of start/end indices\n",
    "    region_to_shade = region_to_shade.reshape((-1, 2))\n",
    "    \n",
    "    return region_to_shade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af81f4f8-3d31-46a8-a016-35d676a45774",
   "metadata": {},
   "source": [
    "## Get spatial map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb692144-e791-4478-9401-b9ac87f023d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_climatology(data_path, variable, time_slice, lat_slice, lon_slice, season=None, iscale=None, mask=True, region_mask=None, region=None):\n",
    "    \n",
    "    if iscale is None:\n",
    "        data = get_data_from_file(data_path, variable, lat_slice, lon_slice).sel(time=time_slice)\n",
    "    \n",
    "    else:\n",
    "        data = get_data_from_file(data_path, variable,lat_slice, lon_slice, iscale).sel(time=time_slice)\n",
    "    \n",
    "    # Check if we are doing an IPCC sub-region\n",
    "    if region is None:\n",
    "      \n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        data = data.where(region_mask==region)\n",
    "    \n",
    "    # Extract unmasked data if no mask provided\n",
    "    if mask is None:\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "       \n",
    "        # Apply mask to data\n",
    "        data = data.where(mask==1) \n",
    "    \n",
    "    \n",
    "    weights = np.cos(np.deg2rad(data.lat))\n",
    "    weights.name = \"weights\"\n",
    "\n",
    "    # Weight: apply weighted lats to data\n",
    "    data_weighted = data.weighted(weights)\n",
    "    \n",
    "    # Extract season if one is given\n",
    "    if season is None:\n",
    "        \n",
    "        # Calculate Annual climatology datasets\n",
    "        data_climatology = data_weighted.mean(dim='time')\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Calculate seasonal climatology datasets; the season is defined by the user as a list of month numbers\n",
    "        data_climatology = data_weighted.sel(time=data.time.dt.month.isin(season)).mean(dim='time')\n",
    "        \n",
    "    return data_climatology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "612d4e99-6546-499b-8e7f-e200ca9c7750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_map(data_path, variable, time_slice, lat_slice, lon_slice, season=None, iscale=None, mask=True, region_mask=None, region=None):\n",
    "    \n",
    "    if iscale is None:\n",
    "        data = get_data_from_file(data_path, variable, lat_slice, lon_slice).sel(time=time_slice)\n",
    "    \n",
    "    else:\n",
    "        data = get_data_from_file(data_path, variable,lat_slice, lon_slice, iscale).sel(time=time_slice)\n",
    "    \n",
    "    # Check if we are doing an IPCC sub-region\n",
    "    if region is None:\n",
    "      \n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        data = data.where(region_mask==region)\n",
    "    \n",
    "    # Extract unmasked data if no mask provided\n",
    "    if mask is None:\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "       \n",
    "        # Apply mask to data\n",
    "        data = data.where(mask==1) #(mask>50,drop=True) - for only landmask in obs only plot \n",
    "    \n",
    "    \n",
    "    # Extract season if one is given\n",
    "    if season is None:\n",
    "        \n",
    "        # Calculate Annual climatology datasets\n",
    "        data_map = data\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Calculate seasonal climatology datasets; the season is defined by the user as a list of month numbers\n",
    "        data_map = data.sel(time=data.time.dt.month.isin(season))\n",
    "        \n",
    "    return data_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88e22aa3-b537-4ff8-90be-903421064a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_map2(data_path, variable, time_slice, lat_slice, lon_slice, season=None, iscale=None, mask=True, region_mask=None, region=None):\n",
    "    \n",
    "    if iscale is None:\n",
    "        data = get_data_from_file(data_path, variable, lat_slice, lon_slice).sel(year=time_slice)\n",
    "    \n",
    "    else:\n",
    "        data = get_data_from_file(data_path, variable,lat_slice, lon_slice, iscale).sel(year=time_slice)\n",
    "    \n",
    "    # Check if we are doing an IPCC sub-region\n",
    "    if region is None:\n",
    "      \n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        data = data.where(region_mask==region)\n",
    "    \n",
    "    # Extract unmasked data if no mask provided\n",
    "    if mask is None:\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "       \n",
    "        # Apply mask to data\n",
    "        data = data.where(mask==1) #(mask>50,drop=True) - for only landmask in obs only plot \n",
    "    \n",
    "    \n",
    "    # Extract season if one is given\n",
    "    if season is None:\n",
    "        \n",
    "        # Calculate Annual climatology datasets\n",
    "        data_map = data\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Calculate seasonal climatology datasets; the season is defined by the user as a list of month numbers\n",
    "        data_map = data.sel(time=data.time.dt.month.isin(season))\n",
    "        \n",
    "    return data_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b823ba-2480-4d5f-a1e3-81ea089d7c30",
   "metadata": {},
   "source": [
    "### Get bias map and K-S test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e44cc04-6ae3-4f9a-93f9-677d245a0d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bias(global_path, regional_path, variable, time_slice, lat_slice, lon_slice, season=None, iscale=None, mask=None, region_mask=None, region=None):\n",
    "    \n",
    "    if iscale is None:\n",
    "        \n",
    "        global_data = get_data_from_file(global_path, variable, lat_slice, lon_slice).sel(time=time_slice)\n",
    "        regional_data = get_data_from_file(regional_path, variable, lat_slice, lon_slice).sel(time=time_slice)\n",
    "    \n",
    "    else:\n",
    "        global_data = get_data_from_file(global_path, variable,lat_slice, lon_slice, iscale).sel(time=time_slice)\n",
    "        regional_data = get_data_from_file(regional_path, variable,lat_slice, lon_slice, iscale).sel(time=time_slice)\n",
    "    \n",
    "    # Check if we are doing an IPCC sub-region\n",
    "    if region is None:\n",
    "      \n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        global_data = global_data.where(region_mask==region)\n",
    "        regional_data = regional_data.where(region_mask==region)\n",
    "    \n",
    "    # Extract unmasked data if no mask provided\n",
    "    if mask is None:\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "       \n",
    "        # Apply mask to data\n",
    "        global_data = global_data.where(mask==1) \n",
    "        regional_data =regional_data.where(mask==1)\n",
    "    \n",
    "    # Extract season if one is given\n",
    "    if season is None:\n",
    "        \n",
    "        # Calculate Annual climatology datasets\n",
    "        global_climatology = global_data.mean(dim='time')\n",
    "        regional_climatology = regional_data.mean(dim='time')\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Calculate seasonal climatology datasets; the season is defined by the user as a list of month numbers\n",
    "        global_climatology = global_data.sel(time=global_data.time.dt.month.isin(season)).mean(dim='time')\n",
    "        regional_climatology = regional_data.sel(time=regional_data.time.dt.month.isin(season)).mean(dim='time')\n",
    "\n",
    "    # Calculate Model bias\n",
    "    bias = (global_climatology - regional_climatology)\n",
    "    \n",
    "    return bias\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47d2b42-360a-4063-a1c2-6db268e9ea56",
   "metadata": {},
   "source": [
    "## relative bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "255b4ad6-4d83-4a2b-9e57-9dd6dcb42ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bias_rel(global_path, regional_path, variable, time_slice, lat_slice, lon_slice, season=None, iscale=None, mask=None, region_mask=None, region=None):\n",
    "    \n",
    "    if iscale is None:\n",
    "        \n",
    "        global_data = get_data_from_file(global_path, variable, lat_slice, lon_slice).sel(time=time_slice)\n",
    "        regional_data = get_data_from_file(regional_path, variable, lat_slice, lon_slice).sel(time=time_slice)\n",
    "    \n",
    "    else:\n",
    "        global_data = get_data_from_file(global_path, variable,lat_slice, lon_slice, iscale).sel(time=time_slice)\n",
    "        regional_data = get_data_from_file(regional_path, variable,lat_slice, lon_slice, iscale).sel(time=time_slice)\n",
    "    \n",
    "    # Check if we are doing an IPCC sub-region\n",
    "    if region is None:\n",
    "      \n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        global_data = global_data.where(region_mask==region)\n",
    "        regional_data = regional_data.where(region_mask==region)\n",
    "    \n",
    "    # Extract unmasked data if no mask provided\n",
    "    if mask is None:\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "       \n",
    "        # Apply mask to data\n",
    "        global_data = global_data.where(mask==1) \n",
    "        regional_data =regional_data.where(mask==1)\n",
    "    \n",
    "    # Extract season if one is given\n",
    "    if season is None:\n",
    "        \n",
    "        # Calculate Annual climatology datasets\n",
    "        global_climatology = global_data.mean(dim='time')\n",
    "        regional_climatology = regional_data.mean(dim='time')\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Calculate seasonal climatology datasets; the season is defined by the user as a list of month numbers\n",
    "        global_climatology = global_data.sel(time=global_data.time.dt.month.isin(season)).mean(dim='time')\n",
    "        regional_climatology = regional_data.sel(time=regional_data.time.dt.month.isin(season)).mean(dim='time')\n",
    "\n",
    "    # Calculate Model bias\n",
    "    bias = (global_climatology - regional_climatology)/regional_climatology*100\n",
    "    \n",
    "    return bias\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9073cb6e-aa10-4e82-82f7-ab35e874dc02",
   "metadata": {},
   "source": [
    "### Get K-S test for two extreme distribution - between regional and global dataset\n",
    "\n",
    "K-S test 2 samples for 3-D xarray data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c6d9ea8-d199-432f-90b6-737207143d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of p-values array: (10, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "def ks_test_3d_xarray(data1, data2):\n",
    "    \"\"\"\n",
    "    Apply K-S test (2 samples) to compare two 3-D xarray DataArrays (time, lat, lon)\n",
    "    with NaN values.\n",
    "    \n",
    "    Parameters:\n",
    "        data1: xarray DataArray of shape (time, lat, lon)\n",
    "        data2: xarray DataArray of shape (time, lat, lon)\n",
    "        \n",
    "    Returns:\n",
    "        p_values: xarray DataArray of p-values with shape (lat, lon)\n",
    "    \"\"\"\n",
    "    # Check if the shapes of the input data are the same\n",
    "    assert data1.shape == data2.shape, \"Input data arrays must have the same shape\"\n",
    "\n",
    "    lat, lon = data1.sizes['lat'], data1.sizes['lon']\n",
    "    p_values = np.zeros((lat, lon))\n",
    "\n",
    "    # Perform K-S test along the time dimension for each (lat, lon) pair\n",
    "    for i in range(lat):\n",
    "        for j in range(lon):\n",
    "            # Extract data for the (lat, lon) pair\n",
    "            d1 = data1[:, i, j].values\n",
    "            d2 = data2[:, i, j].values\n",
    "            \n",
    "            # Remove NaN values and perform K-S test\n",
    "            mask = ~np.isnan(d1) & ~np.isnan(d2)\n",
    "            if np.any(mask):\n",
    "                _, p_value = ks_2samp(d1[mask], d2[mask])\n",
    "                p_values[i, j] = p_value\n",
    "            else:\n",
    "                # If there are no valid values, set p-value to NaN\n",
    "                p_values[i, j] = np.nan\n",
    "\n",
    "    # Create an xarray DataArray for p-values\n",
    "    p_values_xr = xr.DataArray(p_values, dims=('lat', 'lon'), coords={'lat': data1['lat'], 'lon': data1['lon']})\n",
    "    \n",
    "    return p_values_xr\n",
    "\n",
    "# Example usage:\n",
    "# Generate sample data (replace this with your actual data)\n",
    "time_steps = 100\n",
    "lat = 10\n",
    "lon = 10\n",
    "\n",
    "# Create sample data with NaN values\n",
    "data1 = xr.DataArray(np.random.rand(time_steps, lat, lon), dims=('time', 'lat', 'lon'), \n",
    "                     coords={'lat': np.linspace(-90, 90, lat), 'lon': np.linspace(-180, 180, lon)})\n",
    "data2 = xr.DataArray(np.random.rand(time_steps, lat, lon), dims=('time', 'lat', 'lon'), \n",
    "                     coords={'lat': np.linspace(-90, 90, lat), 'lon': np.linspace(-180, 180, lon)})\n",
    "\n",
    "# Introduce NaN values\n",
    "data1.values[0, 2, 3] = np.nan\n",
    "data2.values[0, 2, 3] = np.nan\n",
    "\n",
    "# Apply K-S test\n",
    "p_values = ks_test_3d_xarray(data1, data2)\n",
    "\n",
    "print(\"Shape of p-values array:\", p_values.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581c5d17-40d6-44f6-be5d-f9261c2f2dec",
   "metadata": {},
   "source": [
    "## Manm-kendal Test for difference between global and regional dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0d7dd5f-fcee-416b-9a7e-b696482e71c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mk(regional_path, global_path, variable, time_slice, lat_slice, lon_slice, season=None, iscale=None, mask=None,egion_mask=None, region=None):\n",
    "    \n",
    "    if iscale is None:\n",
    "        \n",
    "        global_data = get_data_from_file(global_path, variable, lat_slice, lon_slice).sel(time=time_slice)\n",
    "        regional_data = get_data_from_file(regional_path, variable, lat_slice, lon_slice).sel(time=time_slice)\n",
    "    \n",
    "    else:\n",
    "        global_data = get_data_from_file(global_path, variable,lat_slice, lon_slice, iscale).sel(time=time_slice)\n",
    "        regional_data = get_data_from_file(regional_path, variable,lat_slice, lon_slice, iscale).sel(time=time_slice)\n",
    "    \n",
    "    # Check if we are doing an IPCC sub-region\n",
    "    if region is None:\n",
    "      \n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        global_data = global_data.where(region_mask==region)\n",
    "        regional_data = regional_data.where(region_mask==region)\n",
    "    \n",
    "    # Extract unmasked data if no mask provided\n",
    "    if mask is None:\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "       \n",
    "        # Apply mask to data\n",
    "        global_data = global_data.where(mask==1) \n",
    "        regional_data =regional_data.where(mask==1)\n",
    "    if season is None:\n",
    "        \n",
    "        # Calculate Annual climatology datasets\n",
    "        regional_data = regional_data\n",
    "        global_data = global_data\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Calculate seasonal climatology datasets; the season is defined by the user as a list of month numbers\n",
    "        regional_data = regional_data.sel(time=regional_data.time.dt.month.isin(season))\n",
    "        global_data = global_data.sel(time=global_data.time.dt.month.isin(season))\n",
    "    \n",
    "    # Calculate Mann Whitney statistic and p-value at each grid point (across time; axis 0)\n",
    "    mann_whitney_stat = mannwhitneyu(regional_data, global_data, axis=0)\n",
    "\n",
    "    # Reformat the pvalue output to have the correct lat/lon coordinates\n",
    "    pvalue_mk_gridded = xr.DataArray(mann_whitney_stat.pvalue, coords={'lat': regional_data['lat'], 'lon': regional_data['lon']}, dims=['lat', 'lon'])\n",
    "    \n",
    "    return pvalue_mk_gridded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ae4cc7-9ed3-4dd4-9247-16aec1fae17c",
   "metadata": {},
   "source": [
    "## Find a commom shape (time, lat, long) between 2 datasets\n",
    "return of time slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94f01cff-ae63-4e88-b354-e11010110f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "def find_common_slices(data1, data2):\n",
    "    \"\"\"\n",
    "    Find the common slices (time, lat, lon) between two 3-D xarray DataArrays (time, lat, lon)\n",
    "    with NaN values and different time steps, numbers of latitudes, and numbers of longitudes.\n",
    "    \n",
    "    Parameters:\n",
    "        data1: xarray DataArray of shape (time, lat, lon)\n",
    "        data2: xarray DataArray of shape (time, lat, lon)\n",
    "        \n",
    "    Returns:\n",
    "        common_slices: dictionary containing slices for each dimension\n",
    "                       {'time_slice': slice, 'lat_slice': slice, 'lon_slice': slice}\n",
    "                       or None if no common slices are found\n",
    "    \"\"\"\n",
    "    # Extract dimensions from both DataArrays\n",
    "    dims1 = set(data1.dims)\n",
    "    dims2 = set(data2.dims)\n",
    "    \n",
    "    # Find the common dimensions\n",
    "    common_dims = dims1.intersection(dims2)\n",
    "    \n",
    "    # Check if time, lat, and lon are common dimensions\n",
    "    if 'time' in common_dims and 'lat' in common_dims and 'lon' in common_dims:\n",
    "        # Find common slices for each dimension\n",
    "        common_slices = {'time_slice': slice(None), 'lat_slice': slice(None), 'lon_slice': slice(None)}\n",
    "        \n",
    "        for dim in ['time', 'lat', 'lon']:\n",
    "            common_coords = np.intersect1d(data1[dim].values, data2[dim].values)\n",
    "            common_slices[dim+'_slice'] = slice(common_coords.min(), common_coords.max()+1)\n",
    "        \n",
    "        return common_slices\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2063001-c3a8-448e-9bab-92074e7ee703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ff60342-0b95-4805-9b63-bded85526acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "def find_common_slices2(data1, data2):\n",
    "    \"\"\"\n",
    "    Find the common slices (year, lat, lon) between two 3-D xarray DataArrays (time, lat, lon)\n",
    "    with NaN values and different time steps, numbers of latitudes, and numbers of longitudes.\n",
    "    \n",
    "    Parameters:\n",
    "        data1: xarray DataArray of shape (year, lat, lon)\n",
    "        data2: xarray DataArray of shape (year, lat, lon)\n",
    "        \n",
    "    Returns:\n",
    "        common_slices: dictionary containing slices for each dimension\n",
    "                       {'year_slice': slice, 'lat_slice': slice, 'lon_slice': slice}\n",
    "                       or None if no common slices are found\n",
    "    \"\"\"\n",
    "    # Extract dimensions from both DataArrays\n",
    "    dims1 = set(data1.dims)\n",
    "    dims2 = set(data2.dims)\n",
    "    \n",
    "    # Find the common dimensions\n",
    "    common_dims = dims1.intersection(dims2)\n",
    "    \n",
    "    # Check if year, lat, and lon are common dimensions\n",
    "    # Check if time, lat, and lon are common dimensions\n",
    "    if 'year' in common_dims and 'lat' in common_dims and 'lon' in common_dims:\n",
    "        # Find common slices for each dimension\n",
    "        common_slices = {'year_slice': slice(None), 'lat_slice': slice(None), 'lon_slice': slice(None)}\n",
    "        \n",
    "        for dim in ['year', 'lat', 'lon']:\n",
    "            common_coords = np.intersect1d(data1[dim].values, data2[dim].values)\n",
    "            common_slices[dim+'_slice'] = slice(common_coords.min(), common_coords.max()+1)\n",
    "        \n",
    "        return common_slices\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd199e3-50e3-47c7-a6fc-4a70e44c9554",
   "metadata": {},
   "source": [
    "## RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9cfcfbe-089b-44f5-94f5-508f9113dfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get RMSE between model simulation and observational dataset. This function assumes area weighting and homogenous variable name between datsets\n",
    "def get_rmse(global_path, regional_path, variable, time_slice, lat_slice, lon_slice, season=None, iscale=None, mask=None, region_mask=None, region=None):\n",
    "\n",
    "    # Get data\n",
    "    # Include SPI scale in data extraction if a scale is provided by the user\n",
    "    if iscale is None:\n",
    "        \n",
    "        global_data = get_data_from_file(global_path, variable, lat_slice, lon_slice).sel(time=time_slice)\n",
    "        regional_data = get_data_from_file(regional_path, variable, lat_slice, lon_slice).sel(time=time_slice)\n",
    "    \n",
    "    else:\n",
    "        global_data = get_data_from_file(global_path, variable,lat_slice, lon_slice, iscale).sel(time=time_slice)\n",
    "        regional_data = get_data_from_file(regional_path, variable,lat_slice, lon_slice, iscale).sel(time=time_slice)\n",
    "     \n",
    "    \n",
    "    if region is None:\n",
    "      \n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        global_data = global_data.where(region_mask==region)\n",
    "        regional_data = regional_data.where(region_mask==region)\n",
    "    # Extract unmasked data if no mask provided\n",
    "    if mask is None:\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "       \n",
    "        # Apply mask to data\n",
    "        global_data = global_data.where(mask==1) \n",
    "        regional_data =regional_data.where(mask==1)\n",
    "    \n",
    "        \n",
    "    if season is None:\n",
    "        global_climatology = global_data.mean(dim='time')\n",
    "        regional_climatology = regional_data.mean(dim='time')\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Calculate seasonal climatology datasets; the season is defined by the user as a list of month numbers\n",
    "        global_climatology = global_data.sel(time=global_data.time.dt.month.isin(season)).mean(dim='time')\n",
    "        regional_climatology = regional_data.sel(time=regional_data.time.dt.month.isin(season)).mean(dim='time')\n",
    "    \n",
    "    \n",
    "    # Calculate weights\n",
    "    weights = np.cos(np.deg2rad(regional_climatology.lat))\n",
    "    weights.name = \"weights\"\n",
    "    weights2d_0 = np.expand_dims(weights.to_numpy(), axis=1)\n",
    "    weights2d = np.repeat(weights2d_0, len(regional_climatology.lon), axis=1)\n",
    "    \n",
    "    # Convert weights to Xarray for input into correlation function\n",
    "    weights2d_xr = xr.DataArray(weights2d, coords={'lat': regional_climatology.lat, 'lon': regional_climatology.lon}, dims=['lat', 'lon'])\n",
    "        \n",
    "    # Calculate Weighted Spatial (i.e. Pattern) Correlation and reduce to 2 decimals\n",
    "    rmse_i = xs.rmse(regional_climatology, global_climatology, dim=['lat', 'lon'], weights=weights2d_xr, skipna=True)\n",
    "    rmse = rmse_i.astype(float).round(2)\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fe742e8-7338-49f4-8181-2c77f751f79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get RMSE between model simulation and observational dataset. This function assumes area weighting and homogenous variable name between datsets\n",
    "def get_mape(global_path, regional_path, variable, time_slice, lat_slice, lon_slice, season=None, iscale=None, mask=None, region_mask=None, region=None):\n",
    "\n",
    "    # Get data\n",
    "    # Include SPI scale in data extraction if a scale is provided by the user\n",
    "    if iscale is None:\n",
    "        \n",
    "        global_data = get_data_from_file(global_path, variable, lat_slice, lon_slice).sel(time=time_slice)\n",
    "        regional_data = get_data_from_file(regional_path, variable, lat_slice, lon_slice).sel(time=time_slice)\n",
    "    \n",
    "    else:\n",
    "        global_data = get_data_from_file(global_path, variable,lat_slice, lon_slice, iscale).sel(time=time_slice)\n",
    "        regional_data = get_data_from_file(regional_path, variable,lat_slice, lon_slice, iscale).sel(time=time_slice)\n",
    "     \n",
    "    \n",
    "    if region is None:\n",
    "      \n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        global_data = global_data.where(region_mask==region)\n",
    "        regional_data = regional_data.where(region_mask==region)\n",
    "    # Extract unmasked data if no mask provided\n",
    "    if mask is None:\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "       \n",
    "        # Apply mask to data\n",
    "        global_data = global_data.where(mask==1) \n",
    "        regional_data =regional_data.where(mask==1)\n",
    "    \n",
    "        \n",
    "    if season is None:\n",
    "        global_climatology = global_data.mean(dim='time')\n",
    "        regional_climatology = regional_data.mean(dim='time')\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Calculate seasonal climatology datasets; the season is defined by the user as a list of month numbers\n",
    "        global_climatology = global_data.sel(time=global_data.time.dt.month.isin(season)).mean(dim='time')\n",
    "        regional_climatology = regional_data.sel(time=regional_data.time.dt.month.isin(season)).mean(dim='time')\n",
    "    \n",
    "    \n",
    "    # Calculate weights\n",
    "    weights = np.cos(np.deg2rad(regional_climatology.lat))\n",
    "    weights.name = \"weights\"\n",
    "    weights2d_0 = np.expand_dims(weights.to_numpy(), axis=1)\n",
    "    weights2d = np.repeat(weights2d_0, len(regional_climatology.lon), axis=1)\n",
    "    \n",
    "    # Convert weights to Xarray for input into correlation function\n",
    "    weights2d_xr = xr.DataArray(weights2d, coords={'lat': regional_climatology.lat, 'lon': regional_climatology.lon}, dims=['lat', 'lon'])\n",
    "        \n",
    "    # Calculate Weighted Spatial (i.e. Pattern) Correlation and reduce to 2 decimals\n",
    "    mape_i = xs.mape(regional_climatology, global_climatology, dim=['lat', 'lon'], weights=weights2d_xr, skipna=True)\n",
    "    mape = mape_i.astype(float).round(2)\n",
    "    \n",
    "    return mape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09379d70-08c5-4477-9d52-f727ff1b0d6a",
   "metadata": {},
   "source": [
    "##MEA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dec3155-fd7e-4980-8787-6d42f333fa6c",
   "metadata": {},
   "source": [
    "## Timming of extremes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "484f579a-e7b4-4430-8bba-f420446fd05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get MEA between model simulation and observational dataset. This function assumes area weighting and homogenous variable name between datsets\n",
    "def get_mae(global_path, regional_path, variable, time_slice, lat_slice, lon_slice, season=None, iscale=None, mask=None, region_mask=None, region=None):\n",
    "\n",
    "    # Get data\n",
    "    # Include SPI scale in data extraction if a scale is provided by the user\n",
    "    if iscale is None:\n",
    "        \n",
    "        global_data = get_data_from_file(global_path, variable, lat_slice, lon_slice).sel(time=time_slice)\n",
    "        regional_data = get_data_from_file(regional_path, variable, lat_slice, lon_slice).sel(time=time_slice)\n",
    "    \n",
    "    else:\n",
    "        global_data = get_data_from_file(global_path, variable,lat_slice, lon_slice, iscale).sel(time=time_slice)\n",
    "        regional_data = get_data_from_file(regional_path, variable,lat_slice, lon_slice, iscale).sel(time=time_slice)\n",
    "     \n",
    "    \n",
    "    if region is None:\n",
    "      \n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        global_data = global_data.where(region_mask==region)\n",
    "        regional_data = regional_data.where(region_mask==region)\n",
    "    # Extract unmasked data if no mask provided\n",
    "    if mask is None:\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "       \n",
    "        # Apply mask to data\n",
    "        global_data = global_data.where(mask==1) \n",
    "        regional_data =regional_data.where(mask==1)\n",
    "    \n",
    "        \n",
    "    if season is None:\n",
    "        global_climatology = global_data.mean(dim='time')\n",
    "        regional_climatology = regional_data.mean(dim='time')\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Calculate seasonal climatology datasets; the season is defined by the user as a list of month numbers\n",
    "        global_climatology = global_data.sel(time=global_data.time.dt.month.isin(season)).mean(dim='time')\n",
    "        regional_climatology = regional_data.sel(time=regional_data.time.dt.month.isin(season)).mean(dim='time')\n",
    "    \n",
    "    # Calculate weights\n",
    "    weights = np.cos(np.deg2rad(regional_climatology.lat))\n",
    "    weights.name = \"weights\"\n",
    "    weights2d_0 = np.expand_dims(weights.to_numpy(), axis=1)\n",
    "    weights2d = np.repeat(weights2d_0, len(regional_climatology.lon), axis=1)\n",
    "    \n",
    "    # Convert weights to Xarray for input into correlation function\n",
    "    weights2d_xr = xr.DataArray(weights2d, coords={'lat': regional_climatology.lat, 'lon': regional_climatology.lon}, dims=['lat', 'lon'])\n",
    "        \n",
    "    # Calculate Weighted Spatial (i.e. Pattern) Correlation and reduce to 2 decimals\n",
    "    mae_i = xs.mae(regional_climatology, global_climatology, dim=['lat', 'lon'], weights=weights2d_xr, skipna=True)\n",
    "    mae = mae_i.astype(float).round(2)\n",
    "    \n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad370bef-b16c-4fa4-bd87-40db9cb3f13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "def timming_grid(precip1, precip2):\n",
    "    \"\"\"\n",
    "    Calculate the percentage similarity between two 3D datasets of monthly annual maxima precipitation\n",
    "    for each grid point (lon, lat).\n",
    "    \n",
    "    Parameters:\n",
    "    - precip1: xarray DataArray, the first 3D dataset (time, lon, lat)\n",
    "    - precip2: xarray DataArray, the second 3D dataset (time, lon, lat)\n",
    "    \n",
    "    Returns:\n",
    "    - similarity: xarray DataArray, a 2D dataset (lon, lat) with percentage similarity per grid point\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure that both datasets have the same dimensions and coordinates\n",
    "    if precip1.shape != precip2.shape:\n",
    "        raise ValueError(\"Datasets must have the same shape and coordinates.\")\n",
    "    \n",
    "    # Compare the two datasets element-wise\n",
    "    matching_values = (precip1 == precip2)\n",
    "    \n",
    "    # Calculate the percentage of matching values for each grid point\n",
    "    r = matching_values.mean(dim=\"year\") * 100  # Averaging along the time dimension\n",
    "    \n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e54a15db-1f7f-4523-8650-bc2e3833aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "def timming_grid2(precip1, precip2):\n",
    "    \"\"\"\n",
    "    Calculate the percentage similarity between two 3D datasets of monthly annual maxima precipitation\n",
    "    for each grid point (lon, lat), handling NaN values.\n",
    "    \n",
    "    Parameters:\n",
    "    - precip1: xarray DataArray, the first 3D dataset (time, lon, lat)\n",
    "    - precip2: xarray DataArray, the second 3D dataset (time, lon, lat)\n",
    "    \n",
    "    Returns:\n",
    "    - similarity: xarray DataArray, a 2D dataset (lon, lat) with percentage similarity per grid point\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure that both datasets have the same dimensions\n",
    "    if precip1.shape != precip2.shape:\n",
    "        raise ValueError(\"Datasets must have the same shape and coordinates.\")\n",
    "    \n",
    "    # Find valid (non-NaN) values in both datasets\n",
    "    valid_mask = ~np.isnan(precip1) & ~np.isnan(precip2)\n",
    "    \n",
    "    # Compare the two datasets where both are valid\n",
    "    matching_values = (precip1 == precip2) & valid_mask\n",
    "    \n",
    "    # Count valid values along the time dimension\n",
    "    valid_counts = valid_mask.sum(dim=\"year\")\n",
    "    \n",
    "    # Compute similarity percentage while avoiding division by zero\n",
    "    similarity = (matching_values.sum(dim=\"year\") / valid_counts) * 100\n",
    "    \n",
    "    # Ensure NaNs remain where there were no valid values\n",
    "    similarity = similarity.where(valid_counts > 0)\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38eb428-0ffe-435c-a350-765aba493787",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clim_labs",
   "language": "python",
   "name": "clim_labs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
